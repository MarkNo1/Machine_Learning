   
\documentclass[11pt]{article}
\renewcommand{\baselinestretch}{1.05}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphics}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem*{surfacecor}{Corollary 1}
\newtheorem{conjecture}{Conjecture} 
\newtheorem{question}{Question} 
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\usepackage{graphicx}


 \begin{document}
 


\title{Homework 2 Machine Learning}
\author{Marco Treglia}
\maketitle

\section{Linear Regression}
Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:

One variable, denoted x, is regarded as the predictor, explanatory, or independent variable.
The other variable, denoted y, is regarded as the response, outcome, or dependent variable.


\section{Mathematical step}
Since we are interested in summarizing the trend between two quantitative variables, the natural question arises â€” "what is the best fitting line?"
In order to examine which of the two lines is a better fit, we first need to introduce some common notation:


$y_{i}$  denotes the observed response for experimental unit $i$

$x_{i}$  denotes the predictor value for experimental unit $i$

$y\hat{}_{i}$  is the predicted response (or fitted value) for experimental unit i


Then, the equation for the best fitting line is:

$$y\hat{}_{i} =b_{0}+b_{1} x_{i}  $$


Where for computing $b_{0} $ and  $b_{1} $ as we need first calculate the correlation $r_{xy}$, the standar deviation $\sigma$ and the mean $\mu$ of  $x_{i} $ and $y_{i}$ :


$$ r_{xy\; }=\frac{\sum_{i=1}^{n}{\left( x_{i}\; -\; \overline{x} \right)\left( y_{i}\; -\; \overline{y} \right)}}{\sqrt{\sum_{i=1}^{n}{\left( x_{i}\; -\; \overline{x} \right)^{2}\sum_{i=1}^{n}{\left( y_{i}\; -\; \overline{y} \right)^{2}}}}}$$

$$\sigma \; \; =\; \sqrt{\frac{1}{N}\sum_{i=1}^{n}{\left( x_{i}\; -\; \mu  \right)^{2}}}$$

Then $b_{1} $ and  $b_{0} $ are : 

$$ b_{1}\; =\; r_{xy}\; \frac{\sigma _{x}}{\sigma _{y}}\;  $$

$$ b_{0}\; =\; \mu _{y}\; -\; b_{1\; }\mu _{x}\; $$


In general, when we predict the actual response to $y_{i}$, we make a prediction error (or residual error) of size:
$$  e_{i} = y_{i} - y\hat{} $$

And for have a better 

\subsection{}





\end{document}